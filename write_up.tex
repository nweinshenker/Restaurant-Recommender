% TODO go back and rewrite the abstract
% TODO write equations for distance metric
% TODO write equations for majority metric
% TODO main activity question about kNN overfitting
% TODO add Voroni diagram in warmup

\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{algorithm, algpseudocode}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{graphicx}
\graphicspath{{../pdf/}{D:}}


\title{Restaurant Recommendations using Collaborative Filtering}
\date{December\\ 2018}
\author{Michael Siem \\ \href{mailto:siem@wisc.edu}{siem@wisc.edu}
	\and Nathan Weinshenker \\ \href{mailto:nweinshenker@wisc.edu}{nweinshenker@wisc.edu}
	\and Jack Long \\ \href{mailto:jlong25@wisc.edu}{jlong25@wisc.edu}}

\begin{document}

\maketitle

\section*{Abstract}
With the growth of ecommerce and internet, predictions of user preferences is a hot topic in todays Machine Learning community. Our goal is to use Yelp's open data set \href {https://www.yelp.com/dataset/challenge} {Yelp Dataset Challenge} as prediction measurement of potential restaurants reviewers. We will go into depth on the implementation of k Nearest Neighbors for building a recommendation system for users of similar preferences.  

\section*{Background Information}

Recommendation systems are a vital tool for providing users with personalized suggestions for items such as movies, music, and restaurants.
Product review data is widely accessible on services such as Spotify, Amazon, and Yelp.
There are two approaches that can be used to make recommendations to users based on this data, memory based, and model based.
Model based systems rely on approximating the data and determining features that are most likely to influence a users rating on an item. 
Matrix Decomposition methods such a SVD are an example of a model based system used to provide recommendations.
Over time models can become outdated and need to be updated to provide a more accurate recommendation.
Memory based systems rely upon maintaining data and finding users or items that are similar to each other.
The advantage of a memory based system is it doesn't learn a model or make any assumptions about the data so if trends change the recommendation system will adapt.
Memory based systems can do a user-user comparison or and item-item comparison.
In the user-user comparison, a user is compared to other users that have rated an item and in the item-item comparison, an item is compared to other items a user has rated to predict the rating the user will give it.
We will further study a memory based recommendation systems, K Nearest Neighbors.
\cite{5}.
	
\subsection*{K Nearest Neighbors}
	
K Nearest Neighbors (k-NN) is a memory based collaborative filter that relies upon comparing features of a user or item to other users or items to receive a recommendation. 
k-NN is also a \textbf{non-parametric} learning algorithm. What non-parametric means is that k-NN has little or no prior knowledge about the data distribution. k-NN  builds the model structure based on the training data and this allows the algorithm to be highly versatile and simple to implement with little to no information about the data \cite{3}. 

\subsubsection*{History of Nearest Neighbors}

In 1992, Paul Resnick and fellow researchers at the University of Minnesota first studied collaborative filtering systems through the GroupLens project\cite{2}\cite{8}.  Collaborative filtering began to arise as a solution for dealing with overload in online information spaces. The first collaborative filtering system was Tapestry: it allowed the user to query for
items in an information domain, such as corporate e-mail, based on
other usersâ€™ opinions or actions\cite{6}. With the growth of the Internet, collaborative filters systems have grown in scope and popularity with big name tech companies like Netflix, Amazon, and Yahoo. Today, collaborative filtering systems are used by many companies to shift through the mountains of readily available data.

\subsubsection{K Nearest Neighbor Pseudocode }
\begin{algorithm}
  \caption{K Nearest Neighbour}
  \begin{algorithmic}
  	\State Classify (X,Y,x) // X:training data, Y: class labels of X, x:unknown sample
	\For{ t = 1 to T}
	\State Compute the distance between $d(X, x)$
	\State Sort all the distances in ascending order
	\EndFor
	\State Find the set Z containing indices for the k smallest distances $d(X_{i},x)$
	\State return the set of k closest points 
  \end{algorithmic}
\end{algorithm}
\textsubscript{Our pseudocode will serve as the map for our derivation.}


\subsubsection{Nearest Neighbor Classification Derivation}
\paragraph*{Terminology and Notation }\cite{4}
\begin{enumerate}
	\item P(A $|$ B), the conditional probability, is the probability of A given B
	\item $h : X \in 0, 1$  \qquad  \qquad \textrm{binary classification rule; 
	"classifier" }
	\item \begin{equation*}
	  \textbf{A}_{h_(x) \neq y} = \begin{cases}
	        0 & h(x) = y 
	        \\ 
	        1 &  otherwise.
	        \end{cases} \qquad \textrm{Loss Function}
	 \end{equation*}
\end{enumerate}
Here, we define $X$ as training data; $Y$ as class labels; and $x$ as the unknown sample; and our distance metric $d$ as the Euclidean distance.
\\ \\
First, k-NN calculates the distances between all  training samples $X_{i}$ and $x$ and sorts all the distances according to:
\begin{equation}
d(x,x_{i}) <= d(x,x_{i+1})
\end{equation}
We then define $N$ as the set of k closest points such that:
\begin{equation}
N_{k}(x) = (x_{1}, y_{1}), (x_{2}, y_{2}), ... , (x_{k}, y_{k}) \qquad where \qquad k \in N
\end{equation} 
This is called a k-neighborhood of $x$  \\ \\
With $N$ as a ordered list of k closest points, we can determine the label of our sample point $x$ w.r.t the majority of classes among neighbors
\begin{equation*}
\hat{p}(Y = y | x) = \frac{1}{k} \sum_{1}^{n} \textbf{A}_{h_(x_{i}) \neq y} 
\end{equation*}
\begin{equation}
\hat{Y}(x) = argmax_{\hat{p}}(Y = y |x)
\end{equation}
Here we compute the percentage of points classified as either 1 or 0 in the neighborhood and take the most frequent class label as the new class label for $x$.
 \newline\newline

\subsubsection*{Classification Decision Rule}
Defining the decision boundaries of our k neighborhoods depends on k.
\\
With k = 1 for k-NN, the predicted class of test sample x is set equal to the true class y of its nearest neighbor, where $m_{i}$ is a nearest neighbor to x if the distance \cite{7}
\begin{equation}
d(m_{i},x) = min_{j}(d(m_{j},x))
\end{equation} 
Once all the test samples have been classified, the classification accuracy is based on the ratio of the number of correctly classified samples to the total number of samples classified, given in the form.
\begin{equation}
Accurary = \sum \frac{correctly classified}{total N}
\end{equation}
		


\subsubsection*{The Distance Function}

Several methods can be used to evaluate the distances between the neighbors and the point of interest.
We will first look at the use of the Euclidean distance or the straight-line distance between the points.

\begin{equation}
d(x_{i},x_{j}) = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2 + ... + (x_{ip} - x_{jp})^2}
\end{equation}
The other distance metric we will look into will be Manhattan distance
\begin{equation}
d(x_{i},x_{j}) = |(x_{i1} - x_{j1})| + |(x_{i2} - x_{j2})| + ... + |(x_{ip} - x_{jp})|
\end{equation}

\section*{Warm-up}

\begin{enumerate}
  \item Below is a diagram of points with classifications of either Red Triangle or Blue Square. 
	\begin{figure}[H]
	  \centering
	  \includegraphics{image/Picture2.png}
	\end{figure}  
  	
  	\begin{enumerate}
  	  \item What is the classification of the green dot for the following k values using the majority classifier?
  	    \begin{center}
          \begin{tabular}{| l | l | l |}
            \hline
            k & Class \\ \hline
            1 & \\
            3 & \\
            5 & \\
			11 & \\            
            \hline
          \end{tabular}
        \end{center} 
        
        \item How does increasing k affect the performance in data sets with a lot of noise?
  	  \end{enumerate} 
  
  \item Use the table below to answer the following questions.
    \begin{center}
      \begin{tabular}{| l | l | l |}
        \hline
        Feature 1 & Feature 2 & Class \\ \hline
        -1 & 1 & - \\
        0 & 1 & + \\
        0 & 2 & - \\
        1 & -1 & - \\
        1 & 0 & + \\
        1 & 2 & + \\
        2 & 2 & - \\
        2 & 3 & + \\
        \hline
      \end{tabular}
    \end{center} 
  	
  	\begin{enumerate}
  	  \item Compute the Manhattan distance to a new point (1,1).
        \begin{center}
          \begin{tabular}{| l | l | l |}
            \hline
            Feature 1 & Feature 2 & Manhattan Distance \\ \hline
            -1 & 1 &  \\
            0 & 1 &  \\
            0 & 2 &  \\
            1 & -1 &  \\
            1 & 0 &  \\
            1 & 2 &  \\
            2 & 2 &  \\
            2 & 3 &  \\
            \hline
        \end{tabular}
      \end{center}
    
    \item Using the majority classifier, what is the label applied to the new point (1,1) for the following k-Values?
      \begin{center}
        \begin{tabular}{| l | l | l |}
          \hline
          k & Class \\ \hline
          1 & \\
          3 & \\
          4 & \\
          \hline
        \end{tabular}
      \end{center}       	  
  	\end{enumerate}

  \item Which of the following will be true both statements? \\

	1 - k-NN is a memory-based approach is that the classifier immediately adapts as we collect new training data. \\
	2 - The computational complexity for classifying new samples grows linearly with the number of samples in the training dataset in the worst-case scenario. \\ \\
		A) 1 \\
		B) 2 \\
		C) 1 and 2 \\
		D) None of these \\

\end{enumerate}

\section*{Main Activity}

The main activity will be evaluating a k-NN as a restaurant recommendation system using the Yelp dataset from \href{https://www.yelp.com/dataset}{https://www.yelp.com/dataset} using KNN.m and users.mat.
The dataset originally contains 5,996,996 reviews for 188,593 businesses in 10 metropolitan areas.
We preprocessed the data set to narrow it down to 1 restaurant and 500 reviews for the restaurant for the activity. The activity will be doing a user-user comparison to the users that have rated the restaurant.
The following features are available for use in the problem. 
The first 3 were taken directly from the user's profile, and the last 4 were derived from the users other reviews.
\begin{enumerate}
\item avg_rating: The average rating the user has given to all businesses they have rated.
\item review_count: The total number of reviews a user has given.
\item useful: Number of "useful" votes the user has given.
\item avg_restaurant_rating: The average rating the user has given to businesses classified as restaurants.
\item cats_pizza: The number of businesses classified as a pizza restaurant the user has been to.
\item cats_bar: The number of businesses classified as a bar the user as been to.
\item cats_italian: The number of businesses classified as an Italian restaurant the user as been to.
\end{enumerate}
The restaurant being evaluated is 


\begin{enumerate}
  \item Navigate to the code section indicated by Problem 1. Uncomment the code section and run the program, it should plot a list of data points and a circle.What do you observe about k - Nearest Neighbors?
  \begin{enumerate}
    \item Change the k value to be k = 1 and k = some large value. How does changing the value of k effect the decision of boundary?
    \item  Depending on if k is small or large, would you expect the model to overfit or underfit the data?
  \end{enumerate}
  \item When deriving K-NN, the distance metric we used was Euclidean distance for determining the distances between the neighbors and the point of interest.
  \begin{enumerate}
    \item If we change the distance function to be the Manhattan distance, how would you expect the decision boundary to change?
    \item What are some reasons you think different distance metrics are used for determining K-NN's decision boundaries?
     \item How you think that chosen distance function can affect the classification accuracy?\cite{9}
  \end{enumerate}
  \begin{enumerate}
  	\item In Part 1, our data set only had 2 features. 
  \end{enumerate}
  
  
\end{enumerate}

\begin{enumerate}


\item Choosing K value

\item Evaluate different distance functions

\item Evaluate different classification metrics

\end{enumerate}


\begin {thebibliography}{999}
\bibliographystyle{plain}
	\bibitem{1}
	Leif E. Peterson
	Scholarpeida
	\href{http://www.scholarpedia.org/article/K-nearest_neighbor}{Scholarpedia}
	\bibitem{2}
	Joseph A. Konstan
    University of Minnesota
    Introduction to Recommender Systems
	\href{https://www-users.cs.umn.edu/~konstan/SIGMOD-2008-Tut.pdf}{UMN SIGMOD-2008}
	\bibitem{3}
	Benjamin Soltoff
    University of Chicago
    Statistical learning: non-parametric methods MACS-3100
	\href{https://cfss.uchicago.edu/persp010_nonparametric.html#objectives}{University Chicago MACS-3100}
	\bibitem{4}
	Lars Schmidt-Thieme
	Institute for Computer Science University of Hildesheim
	 \href{https://www.ismll.uni-hildesheim.de/lehre/ml-07w/skript/ml-2up-03-nearest-neighbor.pdf} {K-NN Derivation}
	 \bibitem{5}
	Yifei Feng and Zhengli Sun
	Stanford University 
	Yelp User Rating
	 \href{http://cs229.stanford.edu/proj2014/Yifei%20Feng,%20Zhengli%20Sun,%20Yelp%20User%20Rating%20Prediction.pdf} {Link towards paper}
	 \bibitem{6}
	Collaborative Filtering Recommender Systems
	By Michael D. Ekstrand, John T. Riedl
	and Joseph A. Konstan
	 \href{http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf} {Group Lens}
	 \bibitem{7}
	 R. Nowak
	 University of Wisconsin-Madison
	 ECE 830 Fall 2010 Statistical Signal Processing
	 \href{http://nowak.ece.wisc.edu/ece830/ece830_lecture24.pdf}{Signal Processing}
	 \bibitem{8}
	 Paul Resnick
	 MIT Coordination Center for Science
	 GroupLens: An Open Architecture for Collaborative
Filtering of Netnews
	\href{http://delivery.acm.org/10.1145/200000/192905/p175-resnick.pdf?ip=72.33.2.208&id=192905&acc=PUBLIC&key=066E7B0AFE2DCD37%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1544639596_64ed1d6ccc2584122e84fe8f012afa64}{Collborative Filtering}
	 \bibitem{9}
	 Li-Yu Hu Min-Wei Huang, Shih-Wen Ke
	 The distance function effect on k-nearest neighbor classification for medical datasets
	\href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978658/}{Distance Function}
\end{thebibliography}

\section*{Solutions}
\subsection*{Warm up Activity}
\subsection*{Main Activity}
\begin{enumerate}
  \item SOLUTION: Answers may vary
  \begin{enumerate}
    \item SOLUTION: Increasing k makes the circle grow since it requires more points to be included in the neighborhood
    \\
    Decreasing k shrinks the circle
    \item SOLUTION:  When K increase , the model becomes simpler because the neighborhoods become larger with more data points. This will make the model under-fit the data.
\\
When K decrease, we encounter the 1-Nearest Neighbor problem so are model should overfit the data because the nearest neighbor should be our sample point.
  \end{enumerate}
  \item When deriving K-NN, the distance metric we used was Euclidean distance for determining the distances between the neighbors and the point of interest.
  \begin{enumerate}
    \item SOLUTION: Euclidean Distance is represented by the $l_{2}$ norm. Changing the distance metric to Manhattan distance should create a more rigid boundary represented by the $l_{2}$ norm.
    \item What are some reasons you think different distance metrics are used for determining K-NN's decision boundaries?
    \item Describe the difference in error rate from Euclidean to Manhattan distance. Does this make sense why 
  \end{enumerate}
  \begin{enumerate}
  	\item SOLUTION: Depending on the dataset, different distance functions can have a better classification accuracy. Depending on the dimensions of a the training data, some points may be farther depending on the distance function.
  \end{enumerate}
  
  
\end{enumerate}



\end{document}


